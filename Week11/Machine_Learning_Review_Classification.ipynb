{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: John Reddy Peasari\n",
    "# Assignment : week11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Evan Carey*\n",
    "\n",
    "*Copyright 2017-2019, BH Analytics, LLC*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The purpose of this section is to go over machine learning! We will focus on classification in the context of python (the scikit-learn module). We will include some general concepts of machine learning as well as the specifics of a few different classification algorithms. For further reading, I highly recommend the free ebook titled 'Introduction to Statistical Learning' by Gareth James. A quick web search should find this book near the top of the search results. For even more in-depth coverage of machine learning algorithms, I recommend the book  'Elements of Statistical Learning' by Trevor Hastie (also free online). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where our outcome (target) variable is discrete with a limited number of possible values, we can use classification algorithms to predict the outcome. Imagine a binary outcome with values of 'Yes' and 'No'. We are interested in predicting the probability that the outcome is either 'Yes' or 'No'. It is also possible to predict outcomes with more than two possible values, but we will focus on the binary case here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set default figure size to be larger \n",
    "## this may only work in matplotlib 2.0+!\n",
    "matplotlib.rcParams['figure.figsize'] = [10.0,6.0]\n",
    "## Enable multiple outputs from jupyter cells\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas version: 1.0.1\n",
      "Matplotlib version: 3.1.3\n",
      "Numpy version: 1.18.1\n",
      "SciKitLearn version: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "## Get Version information\n",
    "print(sys.version)\n",
    "print(\"Pandas version: {0}\".format(pd.__version__))\n",
    "print(\"Matplotlib version: {0}\".format(matplotlib.__version__))\n",
    "print(\"Numpy version: {0}\".format(np.__version__))\n",
    "print(\"SciKitLearn version: {0}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your working directory\n",
    "\n",
    "Set your working directory to make paths easier :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My working directory:\n",
      "C:\\Users\\johnp\\Downloads\\Uploads-week-11\n",
      "My new working directory:\n",
      "C:\\Users\\johnp\\Downloads\\Uploads-week-11\n"
     ]
    }
   ],
   "source": [
    "# Working Directory\n",
    "import os\n",
    "print(\"My working directory:\\n\" + os.getcwd())\n",
    "# Set Working Directory \n",
    "os.chdir(\".\")\n",
    "print(\"My new working directory:\\n\" + os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient Mortality Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset with a binary outcome of mortality as a motivating example.\n",
    "\n",
    "This is a dataset of patients demographics and disease status, with mortality indicated. The dataset is here: \n",
    "\n",
    "`data\\healthcare\\patientAnalyticFile.csv`\n",
    "\n",
    "In practice, you most likely would have created a dataset like this from multiple other files after cleaning, reshaping, and joining them. \n",
    "\n",
    "You can generalize this setup to any situation with a binary outcome, such as estimating the probability of a customer filing a warranty claim, or the probability of a transaction being fraudulent. \n",
    "\n",
    "We will first import this dataset and examine the potential variables to use in our classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Myocardial_infarction</th>\n",
       "      <th>Congestive_heart_failure</th>\n",
       "      <th>Peripheral_vascular_disease</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>...</th>\n",
       "      <th>Metastatic_solid_tumour</th>\n",
       "      <th>HIV</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>First_Appointment_Date</th>\n",
       "      <th>Last_Appointment_Date</th>\n",
       "      <th>DateOfDeath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1962-02-27</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-04-27</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1959-08-18</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2005-11-30</td>\n",
       "      <td>2008-11-02</td>\n",
       "      <td>2008-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1946-02-15</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-11-05</td>\n",
       "      <td>2015-11-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1979-07-27</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1983-02-19</td>\n",
       "      <td>female</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006-09-22</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>19996</td>\n",
       "      <td>1997-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-14</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19997</td>\n",
       "      <td>1984-03-31</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-04-24</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19998</td>\n",
       "      <td>1993-07-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19999</td>\n",
       "      <td>1984-04-17</td>\n",
       "      <td>male</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>20000</td>\n",
       "      <td>1966-05-14</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2012-05-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PatientID DateOfBirth  Gender      Race  Myocardial_infarction  \\\n",
       "0              1  1962-02-27  female  hispanic                      0   \n",
       "1              2  1959-08-18    male     white                      0   \n",
       "2              3  1946-02-15  female     white                      0   \n",
       "3              4  1979-07-27  female     white                      0   \n",
       "4              5  1983-02-19  female  hispanic                      0   \n",
       "...          ...         ...     ...       ...                    ...   \n",
       "19995      19996  1997-12-19  female     other                      0   \n",
       "19996      19997  1984-03-31  female     white                      0   \n",
       "19997      19998  1993-07-04  female     white                      0   \n",
       "19998      19999  1984-04-17    male     other                      0   \n",
       "19999      20000  1966-05-14  female     white                      0   \n",
       "\n",
       "       Congestive_heart_failure  Peripheral_vascular_disease  Stroke  \\\n",
       "0                             0                            0       0   \n",
       "1                             0                            0       0   \n",
       "2                             0                            0       0   \n",
       "3                             0                            0       0   \n",
       "4                             0                            0       0   \n",
       "...                         ...                          ...     ...   \n",
       "19995                         0                            0       0   \n",
       "19996                         0                            0       0   \n",
       "19997                         0                            0       0   \n",
       "19998                         0                            0       0   \n",
       "19999                         0                            0       0   \n",
       "\n",
       "       Dementia  Pulmonary  ...  Metastatic_solid_tumour  HIV  Obesity  \\\n",
       "0             0          0  ...                        0    0        0   \n",
       "1             0          0  ...                        0    0        0   \n",
       "2             0          0  ...                        0    1        0   \n",
       "3             0          1  ...                        0    0        0   \n",
       "4             0          0  ...                        0    0        0   \n",
       "...         ...        ...  ...                      ...  ...      ...   \n",
       "19995         0          0  ...                        0    0        0   \n",
       "19996         0          0  ...                        0    1        0   \n",
       "19997         0          0  ...                        0    0        1   \n",
       "19998         0          0  ...                        0    0        0   \n",
       "19999         0          0  ...                        0    0        0   \n",
       "\n",
       "       Depression  Hypertension  Drugs  Alcohol  First_Appointment_Date  \\\n",
       "0               0             0      0        0              2013-04-27   \n",
       "1               0             1      0        0              2005-11-30   \n",
       "2               0             1      0        0              2011-11-05   \n",
       "3               0             0      0        0              2010-03-01   \n",
       "4               0             1      0        0              2006-09-22   \n",
       "...           ...           ...    ...      ...                     ...   \n",
       "19995           0             0      0        0              2008-06-14   \n",
       "19996           0             1      0        0              2007-04-24   \n",
       "19997           0             1      0        0              2010-10-16   \n",
       "19998           0             1      0        0              2015-01-04   \n",
       "19999           0             0      1        0              2011-04-01   \n",
       "\n",
       "       Last_Appointment_Date  DateOfDeath  \n",
       "0                 2018-06-01          NaN  \n",
       "1                 2008-11-02   2008-11-02  \n",
       "2                 2015-11-13          NaN  \n",
       "3                 2016-01-17   2016-01-17  \n",
       "4                 2018-06-01          NaN  \n",
       "...                      ...          ...  \n",
       "19995             2018-06-01          NaN  \n",
       "19996             2018-06-01          NaN  \n",
       "19997             2018-06-01          NaN  \n",
       "19998             2018-06-01          NaN  \n",
       "19999             2012-05-16          NaN  \n",
       "\n",
       "[20000 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set print limits\n",
    "pd.options.display.max_rows = 10\n",
    "## Import Data\n",
    "df_patient = pd.read_csv('PatientAnalyticFile.csv')\n",
    "df_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a variable to indicate mortality. We can do that based on the abscence of 'date of death':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "19995    0\n",
       "19996    0\n",
       "19997    0\n",
       "19998    0\n",
       "19999    0\n",
       "Name: mortality, Length: 20000, dtype: int32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mortality variable\n",
    "df_patient['mortality'] = \\\n",
    "    np.where(df_patient['DateOfDeath'].isnull(),\n",
    "             0,1)\n",
    "# Examine\n",
    "df_patient['mortality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean         0.354700\n",
       "std          0.478434\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "Name: mortality, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient['mortality'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>Myocardial_infarction</th>\n",
       "      <th>Congestive_heart_failure</th>\n",
       "      <th>Peripheral_vascular_disease</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>Rheumatic</th>\n",
       "      <th>Peptic_ulcer_disease</th>\n",
       "      <th>LiverMild</th>\n",
       "      <th>...</th>\n",
       "      <th>Cancer</th>\n",
       "      <th>LiverSevere</th>\n",
       "      <th>Metastatic_solid_tumour</th>\n",
       "      <th>HIV</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10000.500000</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.043450</td>\n",
       "      <td>0.023950</td>\n",
       "      <td>0.028650</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050450</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>0.033150</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.163450</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.040050</td>\n",
       "      <td>0.079750</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5773.647028</td>\n",
       "      <td>0.208621</td>\n",
       "      <td>0.203873</td>\n",
       "      <td>0.152897</td>\n",
       "      <td>0.166825</td>\n",
       "      <td>0.174401</td>\n",
       "      <td>0.259568</td>\n",
       "      <td>0.110224</td>\n",
       "      <td>0.097762</td>\n",
       "      <td>0.095733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218877</td>\n",
       "      <td>0.220919</td>\n",
       "      <td>0.179033</td>\n",
       "      <td>0.080054</td>\n",
       "      <td>0.369785</td>\n",
       "      <td>0.308229</td>\n",
       "      <td>0.459524</td>\n",
       "      <td>0.196081</td>\n",
       "      <td>0.270913</td>\n",
       "      <td>0.478434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5000.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10000.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15000.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PatientID  Myocardial_infarction  Congestive_heart_failure  \\\n",
       "count  20000.000000           20000.000000              20000.000000   \n",
       "mean   10000.500000               0.045600                  0.043450   \n",
       "std     5773.647028               0.208621                  0.203873   \n",
       "min        1.000000               0.000000                  0.000000   \n",
       "25%     5000.750000               0.000000                  0.000000   \n",
       "50%    10000.500000               0.000000                  0.000000   \n",
       "75%    15000.250000               0.000000                  0.000000   \n",
       "max    20000.000000               1.000000                  1.000000   \n",
       "\n",
       "       Peripheral_vascular_disease        Stroke      Dementia     Pulmonary  \\\n",
       "count                 20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean                      0.023950      0.028650      0.031400      0.072650   \n",
       "std                       0.152897      0.166825      0.174401      0.259568   \n",
       "min                       0.000000      0.000000      0.000000      0.000000   \n",
       "25%                       0.000000      0.000000      0.000000      0.000000   \n",
       "50%                       0.000000      0.000000      0.000000      0.000000   \n",
       "75%                       0.000000      0.000000      0.000000      0.000000   \n",
       "max                       1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          Rheumatic  Peptic_ulcer_disease     LiverMild  ...        Cancer  \\\n",
       "count  20000.000000          20000.000000  20000.000000  ...  20000.000000   \n",
       "mean       0.012300              0.009650      0.009250  ...      0.050450   \n",
       "std        0.110224              0.097762      0.095733  ...      0.218877   \n",
       "min        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "25%        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "50%        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "75%        0.000000              0.000000      0.000000  ...      0.000000   \n",
       "max        1.000000              1.000000      1.000000  ...      1.000000   \n",
       "\n",
       "        LiverSevere  Metastatic_solid_tumour           HIV       Obesity  \\\n",
       "count  20000.000000             20000.000000  20000.000000  20000.000000   \n",
       "mean       0.051450                 0.033150      0.006450      0.163450   \n",
       "std        0.220919                 0.179033      0.080054      0.369785   \n",
       "min        0.000000                 0.000000      0.000000      0.000000   \n",
       "25%        0.000000                 0.000000      0.000000      0.000000   \n",
       "50%        0.000000                 0.000000      0.000000      0.000000   \n",
       "75%        0.000000                 0.000000      0.000000      0.000000   \n",
       "max        1.000000                 1.000000      1.000000      1.000000   \n",
       "\n",
       "         Depression  Hypertension         Drugs       Alcohol     mortality  \n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000  \n",
       "mean       0.106300      0.302900      0.040050      0.079750      0.354700  \n",
       "std        0.308229      0.459524      0.196081      0.270913      0.478434  \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      1.000000      0.000000      0.000000      1.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatientID                  int64\n",
       "DateOfBirth               object\n",
       "Gender                    object\n",
       "Race                      object\n",
       "Myocardial_infarction      int64\n",
       "                           ...  \n",
       "Alcohol                    int64\n",
       "First_Appointment_Date    object\n",
       "Last_Appointment_Date     object\n",
       "DateOfDeath               object\n",
       "mortality                  int32\n",
       "Length: 30, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should change date of birth to be an actual date and calculate age if we want to include it in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        47.247474\n",
       "std         18.145086\n",
       "min         15.753593\n",
       "25%         31.733744\n",
       "50%         47.099247\n",
       "75%         62.924025\n",
       "max         78.743326\n",
       "Name: Age_years, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dateofBirth to date\n",
    "df_patient['DateOfBirth'] = \\\n",
    "    pd.to_datetime(df_patient['DateOfBirth'])\n",
    "# Calculate age in years as of 2015-01-01\n",
    "df_patient['Age_years'] = \\\n",
    "    ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days/365.25)\n",
    "df_patient['Age_years'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow into scikit-learn\n",
    "\n",
    "\n",
    "* There are a number of possible ways to prepare data for modeling in scikit-learn. \n",
    "* You must end up with a numeric ndarray of inputs (X) and a numeric ndarray matrix of the target (Y)\n",
    "* I prefer the following workflow:\n",
    "  * We use pandas to import and clean data\n",
    "  * We use Patsy to create the X and Y ndarrays\n",
    "  * Using categorical transformations (dummy coding) as needed\n",
    "  * Also can generate non-linear terms including splines\n",
    "  * Use scikit-learn for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Patsy to Create the Model Matrices\n",
    "\n",
    "We typically start out with a pandas dataframe for manipulation purposes, then we will use this dataframe as the input to the machine learning library. I created a pandas dataframe above to replicate this process. We will use the dmatrices function from the patsy library to easily generate the design matrices for the machine learning algorithms representing the inputs. THis handles the following:\n",
    "\n",
    "* drops rows with missing data\n",
    "* construct one-hot encoding for categorical variables\n",
    "* optionally adds constant intecercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PatientID', 'DateOfBirth', 'Gender', 'Race', 'Myocardial_infarction',\n",
       "       'Congestive_heart_failure', 'Peripheral_vascular_disease', 'Stroke',\n",
       "       'Dementia', 'Pulmonary', 'Rheumatic', 'Peptic_ulcer_disease',\n",
       "       'LiverMild', 'Diabetes_without_complications',\n",
       "       'Diabetes_with_complications', 'Paralysis', 'Renal', 'Cancer',\n",
       "       'LiverSevere', 'Metastatic_solid_tumour', 'HIV', 'Obesity',\n",
       "       'Depression', 'Hypertension', 'Drugs', 'Alcohol',\n",
       "       'First_Appointment_Date', 'Last_Appointment_Date', 'DateOfDeath',\n",
       "       'mortality', 'Age_years'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mortality ~ Pulmonary + Hypertension + Rheumatic + Paralysis + Myocardial_infarction + Renal + Obesity + Alcohol + Stroke + Dementia + Peptic_ulcer_disease + HIV + Diabetes_without_complications + LiverSevere + Peripheral_vascular_disease + Age_years + Diabetes_with_complications + Gender + Metastatic_solid_tumour + Depression + Congestive_heart_failure + Race + Cancer + LiverMild + Drugs'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create formula for all variables in model\n",
    "vars_remove = ['PatientID','First_Appointment_Date','DateOfBirth',\n",
    "               'Last_Appointment_Date','DateOfDeath','mortality']\n",
    "vars_left = set(df_patient.columns) - set(vars_remove)\n",
    "formula = \"mortality ~ \" + \" + \".join(vars_left)\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frac = 1 ndicates the complete dataset\n",
    "df_patient_sub = \\\n",
    "    df_patient.sample(frac=1,   \n",
    "                     random_state=32)    \n",
    "## use Patsy to create model matrices\n",
    "Y,X = dmatrices(formula,\n",
    "                df_patient_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (20000, 1)\n",
       "  mortality\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "          0\n",
       "          0\n",
       "          1\n",
       "          1\n",
       "          0\n",
       "          0\n",
       "  [19970 rows omitted]\n",
       "  Terms:\n",
       "    'mortality' (column 0)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (20000, 28)\n",
       "  Columns:\n",
       "    ['Intercept',\n",
       "     'Gender[T.male]',\n",
       "     'Race[T.hispanic]',\n",
       "     'Race[T.other]',\n",
       "     'Race[T.white]',\n",
       "     'Pulmonary',\n",
       "     'Hypertension',\n",
       "     'Rheumatic',\n",
       "     'Paralysis',\n",
       "     'Myocardial_infarction',\n",
       "     'Renal',\n",
       "     'Obesity',\n",
       "     'Alcohol',\n",
       "     'Stroke',\n",
       "     'Dementia',\n",
       "     'Peptic_ulcer_disease',\n",
       "     'HIV',\n",
       "     'Diabetes_without_complications',\n",
       "     'LiverSevere',\n",
       "     'Peripheral_vascular_disease',\n",
       "     'Age_years',\n",
       "     'Diabetes_with_complications',\n",
       "     'Metastatic_solid_tumour',\n",
       "     'Depression',\n",
       "     'Congestive_heart_failure',\n",
       "     'Cancer',\n",
       "     'LiverMild',\n",
       "     'Drugs']\n",
       "  Terms:\n",
       "    'Intercept' (column 0)\n",
       "    'Gender' (column 1)\n",
       "    'Race' (columns 2:5)\n",
       "    'Pulmonary' (column 5)\n",
       "    'Hypertension' (column 6)\n",
       "    'Rheumatic' (column 7)\n",
       "    'Paralysis' (column 8)\n",
       "    'Myocardial_infarction' (column 9)\n",
       "    'Renal' (column 10)\n",
       "    'Obesity' (column 11)\n",
       "    'Alcohol' (column 12)\n",
       "    'Stroke' (column 13)\n",
       "    'Dementia' (column 14)\n",
       "    'Peptic_ulcer_disease' (column 15)\n",
       "    'HIV' (column 16)\n",
       "    'Diabetes_without_complications' (column 17)\n",
       "    'LiverSevere' (column 18)\n",
       "    'Peripheral_vascular_disease' (column 19)\n",
       "    'Age_years' (column 20)\n",
       "    'Diabetes_with_complications' (column 21)\n",
       "    'Metastatic_solid_tumour' (column 22)\n",
       "    'Depression' (column 23)\n",
       "    'Congestive_heart_failure' (column 24)\n",
       "    'Cancer' (column 25)\n",
       "    'LiverMild' (column 26)\n",
       "    'Drugs' (column 27)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Testing and Training Samples\n",
    "\n",
    "* The first step is to set aside a test sample of data that will allow us to estimate the generalization error post-fit. This protects against overfitting. \n",
    "* We can use “tuple unpacking” to assign the values (very pythonic :)\n",
    "* We can assign a random seed (state) and fraction to split.\n",
    "\n",
    " For simple random splits, scikit-learn has a function `train_test_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Data into training and sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X,\n",
    "                     np.ravel(Y), # prevents dimensionality error later!\n",
    "                     test_size=0.20,\n",
    "                     random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm the Output Dimensions\n",
    "\n",
    "* We can confirm the dimensions of the data are the same within test and train\n",
    "* The proportion should also be close to the test_size argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Confirm dimensions\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Week 11\n",
    "## Question 1\n",
    "### Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-59f6e91567c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_scores\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m### This table was generated using the code from the cell 88 - 131\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_results' is not defined"
     ]
    }
   ],
   "source": [
    "get_results(result_scores) ### This table was generated using the code from the cell 88 - 131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It was observed that the model \"RandomForest_CV2\" yielded the best training accuracy when compared to other classification models. RandomForest_CV2 \" model was generated using grid search for cross validation where hyperparameter tuning was performed using it. It was observed that max_depth parameter of 7 yielded highest training accuracy. But the model gave quite less testing acuracy when compared to other classification models. Finally, it was observed that the model \"Logistic_SL1_C_auto\" i.e., scaling with regularizaton gave the resonable and high training and testing accuracy when compared to other classification models.GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Fitting Logistic Regression using various solvers for the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_scores_logistic = {}   ## Create dict to store all these results:\n",
    "import time, datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "solvers = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"] ## Varous solvers for logistic regression\n",
    "for i in solvers:                                            ## Interating through each solver and fit a logistic regression with a specific solver\n",
    "    start = datetime.datetime.now()\n",
    "#     time.sleep(10)\n",
    "    clf = LogisticRegression(fit_intercept=True,  solver=i,max_iter = 10000)\n",
    "    clf.fit(X_train,y_train)\n",
    "    end = datetime.datetime.now()\n",
    "    diff = (end - start) ## Getting the execution time of a single model in seconds\n",
    "    diff_seconds = int(diff.total_seconds())\n",
    "    cm = clf.score(X_train,y_train) \n",
    "    result_scores_logistic['Logistic' + \" \" + i] = (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)),diff_seconds) ## Score the Model on Training and Testing Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Function to Print Results\n",
    "def get_results_logistic(x1):\n",
    "    print(\"\\n{0:20}  {1:4}    {2:4} {3:6}\".format('Model','Train','Test',\"Time\"))\n",
    "    print('-------------------------------------------')\n",
    "    for i in x1.keys():\n",
    "        print(\"{0:20}   {1:<6.4}   {2:<6.4} {3:<6.4}\".format(i,x1[i][0],x1[i][1], float(x1[i][2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 \n",
    "### Compare the results of the models in terms of their accuracy (use this as the performance metric to assess generalizability error on the holdout subset) and the time taken (use appropriate timing function). Summarize your results via a table with the following structure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                 Train    Test Time  \n",
      "-------------------------------------------\n",
      "Logistic newton-cg     0.7464   0.7432 0.0   \n",
      "Logistic lbfgs         0.7465   0.743  0.0   \n",
      "Logistic liblinear     0.7463   0.7428 0.0   \n",
      "Logistic sag           0.7464   0.7432 31.0  \n",
      "Logistic saga          0.7464   0.7432 45.0  \n"
     ]
    }
   ],
   "source": [
    "get_results_logistic(result_scores_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 :: Based on the results, which solver yielded the best results? Explain the basis for ranking the models - did you use training subset accuracy? Holdout subset accuracy? Time of execution? All three? Some combination of the three?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It was observed that all the solvers in the logistic regression gave almost exactly accuracy results. Five different solvers were implemented with logistic regression. The solver lbfgs was observed to be the best solver. I have made this conclusion using highest training and testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model: Logistic Regression\n",
    "## Assignment Question 1: Finding the best model among all the classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will start with a basic logistic regression model.\n",
    "* The flow will be similar for other models\n",
    "* Call and save model object with initial parameters, then call the fit() method to perform the optimization\n",
    "* Then call other summary methods post fit to explore the model\n",
    "\n",
    "Check the docs: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import linear model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## Define model parameters\n",
    "## can implement penalties, but check docs for appropriate solver\n",
    "clf = LogisticRegression(fit_intercept=True, # already have the intercept\n",
    "                                      solver='liblinear') # could change to lbfgs!\n",
    "## fit model using data with .fit\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if this is a good model? What makes a good model? Let's make predictions, is this a good model? Which parameters are most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make predictions on training dataset\n",
    "## training error?\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89421304, 0.10578696],\n",
       "       [0.88348911, 0.11651089],\n",
       "       [0.32843137, 0.67156863],\n",
       "       ...,\n",
       "       [0.60711869, 0.39288131],\n",
       "       [0.92560274, 0.07439726],\n",
       "       [0.50209904, 0.49790096]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## can also predict probabilities\n",
    "clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can extract the model coefficients with the .coef_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0869168 , -0.05158328,  0.01197148,  0.01575319, -0.01828697,\n",
       "         0.78973851,  0.04358947,  0.43274266,  1.07918877,  0.01973254,\n",
       "         0.53698977,  0.01932497,  0.063653  ,  0.25968019,  0.57911095,\n",
       "        -0.05854274,  0.51782087, -0.1818799 ,  1.01427688,  0.07134668,\n",
       "         0.44168111,  0.28384971,  0.76761013,  0.06855782, -0.03909414,\n",
       "        -0.03456713,  0.13089186,  0.14541601]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1, 28)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get coefficients\n",
    "clf.coef_\n",
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Model Score (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The concept of error is a bit different for a binary outcome than the continuous case. \n",
    "* We can construct error to be a function of the number of incorrect predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7463125"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get mean accuracy\n",
    "clf.score(X_train,y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our model is accurate about 73% of the time! We can understand what that means by looking at the predictions against the actual outcomes. This is called a confusion matrix.\n",
    "\n",
    "| True negative  | False positive |\n",
    "|----------------|----------------|\n",
    "| False negative | True positive |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## get confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_train,clf.predict(X_train))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.85      0.81     10346\n",
      "         1.0       0.67      0.56      0.61      5654\n",
      "\n",
      "    accuracy                           0.75     16000\n",
      "   macro avg       0.72      0.70      0.71     16000\n",
      "weighted avg       0.74      0.75      0.74     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get classification metrics\n",
    "print(sklearn.metrics.classification_report(y_train,\n",
    "                                            clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7463125"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get accuracy\n",
    "sklearn.metrics.accuracy_score(y_train,\n",
    "                               clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for assessing our model is to use the Kappa statistic (instead of accuracy). The Kappa statistics is a measure of rater agreement, with values between -1 and 1. \n",
    "\n",
    "* A value of 0 indicates the classifier is not better than chance\n",
    "* A value of 1 indicates the classifier is a perfect predictor\n",
    "* A value of -1 indicates the classifier is always wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.424747201090075"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get kappa\n",
    "sklearn.metrics.cohen_kappa_score(y_train,\n",
    "                                  clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Track of Scores Across Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to write a small function that will print the scores from a dict so we can compare the models. I will store the model scores in the dict as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dict to store all these results:\n",
    "result_scores = {}\n",
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Logistic'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Function to Print Results\n",
    "def get_results(x1):\n",
    "    print(\"\\n{0:20}   {1:4}    {2:4}\".format('Model','Train','Test'))\n",
    "    print('-------------------------------------------')\n",
    "    for i in x1.keys():\n",
    "        print(\"{0:20}   {1:<6.4}   {2:<6.4}\".format(i,x1[i][0],x1[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to the Null Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a good score for accuracy? Compared to what? We can consider a null model of simply predicting the most frequent class as a base model. Without any other information, I may predict based simply on the distribution of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.646625)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Null information rate\n",
    "1 - y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikitlearn has a built in dummy classifier that works similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=0, strategy='most_frequent')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.646625"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent',\n",
    "                      random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Null'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "- The next family of models we will consider are called regularized linear regression. \n",
    "- This includes LASSO, Elastic Net, and Ridge regression. \n",
    "- These are penalized forms of a regular linear regression (like a logistic regression). \n",
    "- The basic idea is that we can place a penalty on the estimated coefficients from the general linear model, 'pushing' them towards zero. \n",
    "- If the coefficients are related to the outcome, they will 'push' back against our penalty. \n",
    "- The stronger the relationship (or the stronger the predictor), the stronger they will 'push' back. \n",
    "- The overall effect is that the coefficients are all shrunk towards zero. If the variable is not strongly related to the outcome, it will be shrunk close to zero, or possibly all the way to zero. \n",
    "- This can give us effective variable selection, where the weak variables are eliminated since their coefficients are shrunk all the way to zero. \n",
    "- Depending on how we apply the penalty, variables will either be shrunk all the way to zero (this is called the LASSO), or they will be shrunk to a small number, but still above zero (This is called ridge regression).\n",
    "- We can also apply a mixture of the two penalties, which is called the elastic net regression. \n",
    "- A natural question you might ask is, how do I pick the best model?\n",
    "    + LASSO?\n",
    "    + Ridge regression?\n",
    "    + Elastic net (the mixture of the two)?\n",
    "- Also, how strong of a penalty should I pick?\n",
    "    + A very weak penalty, so it is essentially just a logistic regression?\n",
    "    + A very strong penalty, so almost all the coefficients are equal to zero?\n",
    "    + Maybe something in between?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we implement an L1 penalty using the logistic regression function, we are implementing a LASSO regression. Under an L2 penalty, coefficients can actually be set all the way to 0, thus they are eliminated (spare models, feature selection). It is called L1 because the penalty is linked to the **absolute value** of the coefficient. From the scikit-learn docs, here is the cost function:\n",
    "\n",
    "$$\\min_{w, c} \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[8753, 1593],\n",
       "       [2464, 3190]], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic Regression with l1 penalty\n",
    "## Specify penalty directly as C = 1\n",
    "clf = linear_model.LogisticRegression(penalty='l1',\n",
    "                                      C=1, solver = 'liblinear') # specify penalty\n",
    "clf.fit(X_train,y_train)\n",
    "## get confusion matrix\n",
    "confusion_matrix(y_train,clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7464375"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get accuracy\n",
    "sklearn.metrics.accuracy_score(y_train,clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4250784343866866"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get kappa\n",
    "sklearn.metrics.cohen_kappa_score(y_train,clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.85      0.81     10346\n",
      "         1.0       0.67      0.56      0.61      5654\n",
      "\n",
      "    accuracy                           0.75     16000\n",
      "   macro avg       0.72      0.71      0.71     16000\n",
      "weighted avg       0.74      0.75      0.74     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get classification metrics\n",
    "print(sklearn.metrics.classification_report(y_train,\n",
    "                                            clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n"
     ]
    }
   ],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Logistic_L1_C_1'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about other values for C? We could try 0.1, or 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n",
      "Logistic_L1_C_01       0.7471   0.7425\n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression with l1 penalty\n",
    "## Specify penalty directly as C = 0.1\n",
    "clf = linear_model.LogisticRegression(penalty='l1',\n",
    "                                      C=0.1, solver = 'liblinear') # specify penalty\n",
    "clf.fit(X_train,y_train)\n",
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Logistic_L1_C_01'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n",
      "Logistic_L1_C_01       0.7471   0.7425\n",
      "Logistic_L1_C_10       0.7463   0.743 \n"
     ]
    }
   ],
   "source": [
    "## Logistic Regression with l1 penalty\n",
    "## Specify penalty directly as C = 0.1\n",
    "clf = linear_model.LogisticRegression(penalty='l1',\n",
    "                                      C=10, solver = 'liblinear') # specify penalty\n",
    "clf.fit(X_train,y_train)\n",
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Logistic_L1_C_10'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Parameters via Cross Validation\n",
    "\n",
    "We should be validating this parameter 'C' somehow. We should not be using the test data to do that however! We need another dataset, called the validation dataset. We could further split our training data into validation and training. Another option would be to implement k-folds cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=20, class_weight=None, cv=5, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None,\n",
       "                     penalty='l1', random_state=None, refit=True, scoring=None,\n",
       "                     solver='liblinear', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Select the alpha through cross validation (k-folds leave one out)\n",
    "# auto generate 20 values between 1e-4 and 1e4 on log scale\n",
    "clf = linear_model.LogisticRegressionCV(cv=5,\n",
    "                                        Cs=20, ## takes awhile to fit 20 models!\n",
    "                                        penalty='l1',\n",
    "                                        solver='liblinear') \n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.08858668])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## how many C's were fit?\n",
    "clf.Cs\n",
    "## which C's were fit?\n",
    "clf.Cs_\n",
    "## Which C was 'best'? \n",
    "clf.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n",
      "Logistic_L1_C_01       0.7471   0.7425\n",
      "Logistic_L1_C_10       0.7463   0.743 \n",
      "Logistic_L1_C_auto     0.7471   0.7418\n"
     ]
    }
   ],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Logistic_L1_C_auto'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling / Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We should consider scaling when we use regularization methods. \n",
    "* We can construct a pipeline to avoid having to apply the same transformation over and over again.\n",
    "* We must use the StandardScaler() function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scale',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('LASSO',\n",
       "                 LogisticRegressionCV(Cs=20, class_weight=None, cv=5,\n",
       "                                      dual=False, fit_intercept=True,\n",
       "                                      intercept_scaling=1.0, l1_ratios=None,\n",
       "                                      max_iter=100, multi_class='auto',\n",
       "                                      n_jobs=None, penalty='l1',\n",
       "                                      random_state=None, refit=True,\n",
       "                                      scoring=None, solver='liblinear',\n",
       "                                      tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LASSO (L1) regression, set alpha\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "## set our transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## set our model\n",
    "clf = linear_model.LogisticRegressionCV(cv=5,\n",
    "                                        Cs=20, ## takes awhile to fit 20 models!\n",
    "                                        penalty='l1',\n",
    "                                        solver='liblinear') \n",
    "## put together in pipeline\n",
    "pipe1 = Pipeline([(\"scale\", scaler),\n",
    "                  (\"LASSO\", clf)])\n",
    "pipe1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract the elements from the pipline using their names. By calling `named_steps`, you get a dict of the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scale': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'LASSO': LogisticRegressionCV(Cs=20, class_weight=None, cv=5, dual=False,\n",
       "                      fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                      max_iter=100, multi_class='auto', n_jobs=None,\n",
       "                      penalty='l1', random_state=None, refit=True, scoring=None,\n",
       "                      solver='liblinear', tol=0.0001, verbose=0)}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=20, class_weight=None, cv=5, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None,\n",
       "                     penalty='l1', random_state=None, refit=True, scoring=None,\n",
       "                     solver='liblinear', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.named_steps['LASSO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01274275])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.named_steps['LASSO'].C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate this like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n",
      "Logistic_L1_C_01       0.7471   0.7425\n",
      "Logistic_L1_C_10       0.7463   0.743 \n",
      "Logistic_L1_C_auto     0.7471   0.7418\n",
      "Logistic_SL1_C_auto    0.7469   0.744 \n"
     ]
    }
   ],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Logistic_SL1_C_auto'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,pipe1.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,pipe1.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding model complexity with interactions and polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can add polynomials as well as interactions using PolynomialFeatures. \n",
    "* We must give the degree argument. Polynomials up to that degree will be considered, and interactions between d-1 terms.\n",
    "* This is a lot of parameters added into the model! That is why we are using LASSO to shrink some and avoid overfitting…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This takes awhile to run! \n",
    "## try multiple polynomials with a LASSO regulaizer\n",
    "## use pipeline for pre-processing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "## set our transformation\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## polynomials\n",
    "poly_feat = PolynomialFeatures(degree=2,include_bias=False)\n",
    "\n",
    "## set our model\n",
    "clf = linear_model.LogisticRegressionCV(cv=3,\n",
    "                                        Cs=5,\n",
    "                                        penalty='l1',\n",
    "                                        solver='liblinear') \n",
    "## put together in pipeline\n",
    "pipe2 = Pipeline([(\"scale\", scaler),\n",
    "                  (\"poly\",poly_feat),\n",
    "                  (\"LASSO\", clf)])\n",
    "# pipe2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "# result_scores['LogisticL1_SP_C_auto'] = \\\n",
    "#             (sklearn.metrics.accuracy_score(y_train,pipe1.predict(X_train)),\n",
    "#              sklearn.metrics.accuracy_score(y_test,pipe1.predict(X_test)))\n",
    "# get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "- Another popular classification algorithm is the random forest. \n",
    "- In order to understand a random forest, you should first think about a decision tree. \n",
    "- We can consider modeling data simply by making cutpoints on our predictors, then splitting the decision of the outcome. \n",
    "- Visually, that might look like this in the context of our data:\n",
    "- If age < 40, deny credit card\n",
    "- If age > 40, then:\n",
    "    + If income > 5, accept credit card\n",
    "    + If income < 5, accept credit card\n",
    "    \n",
    "- Decision trees have a very nice appeal in that they are easy to understand and visualize. You can simply make a score card, and a human could easily make a decision on whether the outcome is yes or no. \n",
    "- But they are not very accurate or flexible individually! How can we have a good model from a single decision tree? It seems unlikely. \n",
    "- But what if we created many different decision trees, based on different subsets of the data? \n",
    "- We could take random samples of the data, then get an optimal decision tree using a subset of the predictors for each sample. \n",
    "- Each individual tree isn't that great, but perhaps the population of all those trees (the ensemble) would be good?\n",
    "- That is the intuition behind a random forest!\n",
    "\n",
    "The parameters of interest for tuning are `n_estimators` and `max_features`.   \n",
    "  \n",
    "`n_estimators` is the number of trees in the forest. Generally, the larger the number of tree, the better the prediction and more stable the algorithm. However, the algorihm takes longer the more trees we have. \n",
    "  \n",
    "`max_features` is the maximum number of features (variables) to consider for each tree split. Not every tree will use every parameter. It is often suggested to use the square root of the number of features for classification here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features=10,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[10331,    15],\n",
       "       [   21,  5633]], dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Fit Random Forest\n",
    "## Random Forests\n",
    "from sklearn import ensemble\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=100, \n",
    "                                      max_features=10,\n",
    "                                      random_state=42)\n",
    "clf.fit(X_train,y_train)\n",
    "## get confusion matrix\n",
    "confusion_matrix(y_train,clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n",
      "Logistic_L1_C_01       0.7471   0.7425\n",
      "Logistic_L1_C_10       0.7463   0.743 \n",
      "Logistic_L1_C_auto     0.7471   0.7418\n",
      "Logistic_SL1_C_auto    0.7469   0.744 \n",
      "RandomForest_noCV      0.9978   0.6897\n"
     ]
    }
   ],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['RandomForest_noCV'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Manual Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no RandomForestCV function....what to do? \n",
    "\n",
    "\n",
    "We can specify a grid search across a range of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=32,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_features': (5, 10, 15, 20),\n",
       "                         'n_estimators': (50, 100, 200, 300)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "## specify grid\n",
    "parameters = {'n_estimators':(50,100,200,300),\n",
    "              'max_features':(5,10,15,20)}\n",
    "## specify model without hyperparameters\n",
    "rf_model = ensemble.RandomForestClassifier(random_state=32)\n",
    "## specify search with model\n",
    "clf = GridSearchCV(rf_model,\n",
    "                   parameters,\n",
    "                   cv=5,\n",
    "                   return_train_score=True)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 20, 'n_estimators': 300}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## explore best hyperparameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n",
      "Logistic_L1_C_01       0.7471   0.7425\n",
      "Logistic_L1_C_10       0.7463   0.743 \n",
      "Logistic_L1_C_auto     0.7471   0.7418\n",
      "Logistic_SL1_C_auto    0.7469   0.744 \n",
      "RandomForest_noCV      0.9978   0.6897\n",
      "RandomForest_CV        0.9978   0.6923\n"
     ]
    }
   ],
   "source": [
    "## add model score\n",
    "## Score the Model on Training and Testing Set\n",
    "result_scores['RandomForest_CV'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.68338914,  1.23517108,  2.48894906,  3.73244753,  0.94711595,\n",
       "         1.78928566,  3.499506  ,  5.91873088,  1.59184256,  2.77658029,\n",
       "         4.822787  ,  7.26125779,  1.5592968 ,  3.43994203,  6.30519004,\n",
       "        10.00100307]),\n",
       " 'std_fit_time': array([0.02263154, 0.02039614, 0.02435061, 0.01465938, 0.08078149,\n",
       "        0.04175493, 0.0436425 , 0.45475914, 0.08849764, 0.24534329,\n",
       "        0.13617393, 0.11920663, 0.08232472, 0.49161664, 0.38566691,\n",
       "        0.78427924]),\n",
       " 'mean_score_time': array([0.04139657, 0.06760674, 0.13747859, 0.20587053, 0.03794179,\n",
       "        0.07189326, 0.13373647, 0.22506013, 0.04562106, 0.07640953,\n",
       "        0.13867826, 0.20969014, 0.03836212, 0.08234425, 0.14727273,\n",
       "        0.20855589]),\n",
       " 'std_score_time': array([0.00467418, 0.00107072, 0.00135697, 0.00209902, 0.00468266,\n",
       "        0.00779078, 0.00092968, 0.01742322, 0.00560766, 0.0056185 ,\n",
       "        0.01120647, 0.01097023, 0.00431585, 0.0174123 , 0.01505431,\n",
       "        0.00956511]),\n",
       " 'param_max_features': masked_array(data=[5, 5, 5, 5, 10, 10, 10, 10, 15, 15, 15, 15, 20, 20, 20,\n",
       "                    20],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[50, 100, 200, 300, 50, 100, 200, 300, 50, 100, 200,\n",
       "                    300, 50, 100, 200, 300],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_features': 5, 'n_estimators': 50},\n",
       "  {'max_features': 5, 'n_estimators': 100},\n",
       "  {'max_features': 5, 'n_estimators': 200},\n",
       "  {'max_features': 5, 'n_estimators': 300},\n",
       "  {'max_features': 10, 'n_estimators': 50},\n",
       "  {'max_features': 10, 'n_estimators': 100},\n",
       "  {'max_features': 10, 'n_estimators': 200},\n",
       "  {'max_features': 10, 'n_estimators': 300},\n",
       "  {'max_features': 15, 'n_estimators': 50},\n",
       "  {'max_features': 15, 'n_estimators': 100},\n",
       "  {'max_features': 15, 'n_estimators': 200},\n",
       "  {'max_features': 15, 'n_estimators': 300},\n",
       "  {'max_features': 20, 'n_estimators': 50},\n",
       "  {'max_features': 20, 'n_estimators': 100},\n",
       "  {'max_features': 20, 'n_estimators': 200},\n",
       "  {'max_features': 20, 'n_estimators': 300}],\n",
       " 'split0_test_score': array([0.6784375, 0.6790625, 0.6775   , 0.6775   , 0.68625  , 0.6859375,\n",
       "        0.683125 , 0.68375  , 0.6946875, 0.695    , 0.6959375, 0.6921875,\n",
       "        0.69375  , 0.6928125, 0.6940625, 0.69625  ]),\n",
       " 'split1_test_score': array([0.6703125, 0.6709375, 0.6671875, 0.6659375, 0.683125 , 0.6840625,\n",
       "        0.68375  , 0.68125  , 0.689375 , 0.6921875, 0.68875  , 0.689375 ,\n",
       "        0.6890625, 0.6825   , 0.685625 , 0.68875  ]),\n",
       " 'split2_test_score': array([0.668125 , 0.6678125, 0.6634375, 0.6646875, 0.673125 , 0.6734375,\n",
       "        0.676875 , 0.671875 , 0.680625 , 0.68125  , 0.680625 , 0.6803125,\n",
       "        0.6815625, 0.6878125, 0.683125 , 0.684375 ]),\n",
       " 'split3_test_score': array([0.6825   , 0.678125 , 0.6790625, 0.680625 , 0.6853125, 0.6878125,\n",
       "        0.6903125, 0.693125 , 0.68875  , 0.6928125, 0.6928125, 0.6934375,\n",
       "        0.6934375, 0.69375  , 0.6928125, 0.691875 ]),\n",
       " 'split4_test_score': array([0.676875 , 0.675625 , 0.67625  , 0.676875 , 0.68625  , 0.693125 ,\n",
       "        0.69125  , 0.6934375, 0.695    , 0.6990625, 0.6965625, 0.695    ,\n",
       "        0.699375 , 0.6984375, 0.695625 , 0.7      ]),\n",
       " 'mean_test_score': array([0.67525  , 0.6743125, 0.6726875, 0.673125 , 0.6828125, 0.684875 ,\n",
       "        0.6850625, 0.6846875, 0.6896875, 0.6920625, 0.6909375, 0.6900625,\n",
       "        0.6914375, 0.6910625, 0.69025  , 0.69225  ]),\n",
       " 'std_test_score': array([0.00530109, 0.00430025, 0.00620169, 0.00651621, 0.00497651,\n",
       "        0.00646988, 0.00526041, 0.00805741, 0.00522165, 0.00591806,\n",
       "        0.00585302, 0.00521192, 0.00592466, 0.00545077, 0.00494264,\n",
       "        0.00548578]),\n",
       " 'rank_test_score': array([13, 14, 16, 15, 12, 10,  9, 11,  8,  2,  5,  7,  3,  4,  6,  1]),\n",
       " 'split0_train_score': array([0.99664062, 0.99804688, 0.9984375 , 0.9984375 , 0.99703125,\n",
       "        0.99820312, 0.9984375 , 0.9984375 , 0.9971875 , 0.9984375 ,\n",
       "        0.9984375 , 0.9984375 , 0.99726563, 0.9984375 , 0.9984375 ,\n",
       "        0.9984375 ]),\n",
       " 'split1_train_score': array([0.99648437, 0.99789063, 0.99804688, 0.99804688, 0.996875  ,\n",
       "        0.99796875, 0.99804688, 0.99804688, 0.996875  , 0.99796875,\n",
       "        0.99804688, 0.99804688, 0.99703125, 0.99796875, 0.99804688,\n",
       "        0.99804688]),\n",
       " 'split2_train_score': array([0.99617188, 0.99773437, 0.9978125 , 0.9978125 , 0.99664062,\n",
       "        0.99773437, 0.9978125 , 0.9978125 , 0.99664062, 0.99773437,\n",
       "        0.9978125 , 0.9978125 , 0.9965625 , 0.99773437, 0.9978125 ,\n",
       "        0.9978125 ]),\n",
       " 'split3_train_score': array([0.99609375, 0.99820312, 0.99828125, 0.99828125, 0.99609375,\n",
       "        0.99820312, 0.99828125, 0.99828125, 0.99671875, 0.99820312,\n",
       "        0.99828125, 0.99828125, 0.9971875 , 0.99820312, 0.99828125,\n",
       "        0.99828125]),\n",
       " 'split4_train_score': array([0.99648437, 0.998125  , 0.99828125, 0.99828125, 0.99679688,\n",
       "        0.99820312, 0.99828125, 0.99828125, 0.99710937, 0.99820312,\n",
       "        0.99828125, 0.99828125, 0.99710937, 0.998125  , 0.99828125,\n",
       "        0.99828125]),\n",
       " 'mean_train_score': array([0.996375  , 0.998     , 0.99817187, 0.99817187, 0.9966875 ,\n",
       "        0.9980625 , 0.99817187, 0.99817187, 0.99690625, 0.99810937,\n",
       "        0.99817187, 0.99817187, 0.99703125, 0.99809375, 0.99817187,\n",
       "        0.99817187]),\n",
       " 'std_train_score': array([0.00020729, 0.00016829, 0.00021875, 0.00021875, 0.0003225 ,\n",
       "        0.0001875 , 0.00021875, 0.00021875, 0.0002131 , 0.00023902,\n",
       "        0.00021875, 0.00021875, 0.00024705, 0.0002349 , 0.00021875,\n",
       "        0.00021875])}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add one more adjustment to the RandomForest. Since we are still overfitting, let's try optimizing the depth of the trees..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=20,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=32,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'max_depth': (2, 5, 7, 10, 20)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "## specify grid\n",
    "parameters2 = {'max_depth':(2,5,7,10,20)}\n",
    "## specify model without hyperparameters\n",
    "rf_model = ensemble.RandomForestClassifier(max_features=20,\n",
    "                                           n_estimators=100,\n",
    "                                           random_state=32)\n",
    "## specify search with model\n",
    "clf = GridSearchCV(rf_model,\n",
    "                   parameters2,\n",
    "                   cv=5,\n",
    "                   return_train_score=True)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## explore best hyperparameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Test\n",
      "-------------------------------------------\n",
      "Logistic               0.7463   0.7428\n",
      "Null                   0.6466   0.64  \n",
      "Logistic_L1_C_1        0.7464   0.7425\n",
      "Logistic_L1_C_01       0.7471   0.7425\n",
      "Logistic_L1_C_10       0.7463   0.743 \n",
      "Logistic_L1_C_auto     0.7471   0.7418\n",
      "Logistic_SL1_C_auto    0.7469   0.744 \n",
      "RandomForest_noCV      0.9978   0.6897\n",
      "RandomForest_CV        0.9978   0.6923\n",
      "RandomForest_CV2       0.7542   0.737 \n"
     ]
    }
   ],
   "source": [
    "## add model score\n",
    "## Score the Model on Training and Testing Set\n",
    "result_scores['RandomForest_CV2'] = \\\n",
    "            (sklearn.metrics.accuracy_score(y_train,clf.predict(X_train)),\n",
    "             sklearn.metrics.accuracy_score(y_test,clf.predict(X_test)))\n",
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
